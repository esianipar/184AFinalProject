{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61446,"databundleVersionId":6962461,"sourceType":"competition"},{"sourceId":7152564,"sourceType":"datasetVersion","datasetId":4103654}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q segmentation_models_pytorch transformers accelerate datasets\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:20:42.063432Z","iopub.execute_input":"2023-12-08T20:20:42.063757Z","iopub.status.idle":"2023-12-08T20:21:02.829189Z","shell.execute_reply.started":"2023-12-08T20:20:42.063726Z","shell.execute_reply":"2023-12-08T20:21:02.828129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os.path import dirname, abspath\nimport os\nimport copy\nimport cv2\nimport glob\nimport torch\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nimport segmentation_models_pytorch as smp\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport gc\nimport torch.nn.functional as F\nfrom collections import defaultdict\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport pandas as pd\nimport albumentations as A\nfrom typing import Optional, List, Dict\nfrom torch.cuda import amp\nimport transformers\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"e6451af7-0ccb-4a63-b4c6-019c17389d04","_cell_guid":"0b173fbf-73c5-49db-b732-81acd3337d44","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-08T20:21:02.831473Z","iopub.execute_input":"2023-12-08T20:21:02.831813Z","iopub.status.idle":"2023-12-08T20:21:12.124356Z","shell.execute_reply.started":"2023-12-08T20:21:02.831784Z","shell.execute_reply":"2023-12-08T20:21:12.123500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class to store information","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed          = 42\n    debug         = False\n    saved_model_path = \"/kaggle/working\"\n    train_bs      = 32\n    valid_bs      = 32\n    img_size      = [512, 512]\n    train_groups  = [\"kidney_1_dense\", \"kidney_2\"]\n    valid_groups  = [\"kidney_3_sparse\"]\n    scheduler     = \"CosineAnnealingLR\"\n    n_fold        = 5\n    num_classes   = 1\n    n_accumulate  = max(1, 64//train_bs)\n    data_dir = os.path.join(dirname(os.getcwd()), \"input\", \"blood-vessel-segmentation\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    pretrained = {\n        \"unet_parameters\": os.path.join(dirname(os.getcwd()), \"input\", \"saved-models/unet-epoch10-lr0.0001.pth\"),\n        \"sam_parameters\": os.path.join(dirname(os.getcwd()), \"input\", \"saved-models/medsam_pretrained_parameters.pth\"),\n        \"processor\": os.path.join(dirname(os.getcwd()), \"input\", \"saved-models/preprocessor_config.json\"),\n    }\n    \n    data_transforms = {\n        \"train\": A.Compose([\n            A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n            A.HorizontalFlip(p=0.5),\n        ], p=1.0),\n        \"valid\": A.Compose([\n            A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n    }\n    \n    data_augmentation = {\n        \"default\": A.Compose([\n            A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n            A.RandomCrop(height=256, width=256, always_apply=True),\n            A.RandomBrightness(p=1),\n            A.OneOf(\n                [\n                    A.Blur(blur_limit=3, p=1),\n                    A.MotionBlur(blur_limit=3, p=1),\n                ],\n                p=0.9,\n            ),\n\n        ])\n    }\n    \n    sam_transformations = {\n        \"default\": A.Compose([\n            A.Resize(*[1024, 1024], interpolation=cv2.INTER_NEAREST),\n#             A.HorizontalFlip(p=0.5),\n#             A.VerticalFlip(p=0.5)\n#             A.RandomBrightness(p=1),\n#             A.OneOf(\n#                 [\n#                     A.Blur(blur_limit=3, p=1),\n#                     A.MotionBlur(blur_limit=3, p=1),\n#                 ],\n#                 p=0.9,\n#             )\n        ])\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:12.125498Z","iopub.execute_input":"2023-12-08T20:21:12.125953Z","iopub.status.idle":"2023-12-08T20:21:12.172281Z","shell.execute_reply.started":"2023-12-08T20:21:12.125925Z","shell.execute_reply":"2023-12-08T20:21:12.171258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting seed to ensure reproducibility","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(CFG.seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:12.174737Z","iopub.execute_input":"2023-12-08T20:21:12.175036Z","iopub.status.idle":"2023-12-08T20:21:12.198038Z","shell.execute_reply.started":"2023-12-08T20:21:12.175010Z","shell.execute_reply":"2023-12-08T20:21:12.197234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the mask and kidney slice images","metadata":{}},{"cell_type":"code","source":"def list_files_in_directory(directory_path: str) -> List[str]:\n    direc = []\n    for root, dirs, files in os.walk(directory_path):\n        for dire in dirs:\n            if dire in [\"labels\", \"images\"]:\n                continue \n            file_path = os.path.join(root, dire)\n            direc.append(file_path)\n    return direc\n\ndef count_total_img(folders: List[str]) -> Dict:\n    \"\"\"\n    Count numbers of samples per training folder\n    \"\"\"\n    sub_f = [\"images\", \"labels\"]\n    path = []\n    total_files = []\n    random_dir = random.choice(folders)\n    for dire in folders:\n        for subf in sub_f:\n            if (dire == \"/kaggle/input/blood-vessel-segmentation/train/kidney_3_dense\") & (subf == \"images\"):\n                continue \n            \n            _dir = dire + \"/\" + subf\n            total_sample = len(os.listdir(_dir))\n            print(f\"{_dir}: {total_sample}\")\n            path.append(_dir)\n            total_files.append(total_sample)\n    obj = {\n        \"path\": path,\n        \"total_files\":total_files\n    }\n    return obj\n\ndef display_random_img(folders: Dict):\n    \"\"\"\n    Display random slice image from data and its corresponding mask\n    \"\"\"\n    _paths = list(zip(folders['path'], folders['total_files']))\n    _path_tup = random.choice(_paths)\n    split_text = _path_tup[0].split(\"/\")\n\n    img_no = random.choice(range(_path_tup[1]))\n    img_no = f\"{img_no:04}\"\n    random_img_no = str(img_no)\n    _IMG_PATH =  _path_tup[0] + '/' + random_img_no +\".tif\" \n    IMG_PATH = _IMG_PATH.replace(\"labels\",\"images\")\n    LABEL_PATH = IMG_PATH.replace(\"images\",\"labels\")\n    \n    if \"kidney_3_dense\" in split_text:\n        IMG_PATH = IMG_PATH\n        LABEL_PATH = IMG_PATH.replace(\"kidney_3_dense\",\"kidney_3_sparse\").replace(\"labels\",\"images\")\n        \n    try:\n        print(IMG_PATH)\n        print(LABEL_PATH)\n        _slice = plt.imread(IMG_PATH)\n        _mask = plt.imread(LABEL_PATH)\n        \n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.imshow(_slice)\n        print(_slice.shape)\n        print(_mask.shape)\n        plt.title(f'3D image slice: {img_no}')\n        plt.subplot(1, 2, 2)\n        plt.imshow(_mask)\n        plt.title(f'Mask: {img_no}')\n        plt.show()\n    except Exception as e:\n        print(f\"An error occurred:{e}\")\n        \ndef dice_coeff(prediction, target):\n    mask = np.zeros_like(prediction)\n    mask[prediction >= 0.5] = 1\n\n    inter = np.sum(mask * target)\n    union = np.sum(mask) + np.sum(target)\n    epsilon = 1e-6\n    result = np.mean(2 * inter / (union + epsilon))\n    return result\n        \ndef remove_small_objects(img, min_size):\n    # Find all connected components (labels)\n    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n\n    # Create a mask where small objects are removed\n    new_img = np.zeros_like(img)\n    for label in range(1, num_labels):\n        if stats[label, cv2.CC_STAT_AREA] >= min_size:\n            new_img[labels == label] = 255\n\n    return new_img\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    if rle == '':\n        rle = '1 0'\n    return rle\n\ndef dice_coeff_batch(predictions, targets):\n    # Initialize an array to store dice coefficients for each pair in the batch\n    dice_coeffs = np.zeros(predictions.shape[0])\n\n    for i in range(predictions.shape[0]):\n        # Extract the single prediction and target pair from the batch\n        prediction = predictions[i]\n        target = targets[i]\n\n        # Create a binary mask for the prediction\n        mask = np.zeros_like(prediction)\n        mask[prediction >= 0.5] = 1\n        \n        # Compute intersection and union\n        inter = np.sum(mask * target)\n        union = np.sum(mask) + np.sum(target)\n        # Add a small epsilon to avoid division by zero\n        epsilon = 1e-6\n        # Compute Dice coefficient for the current pair\n        dice_coeffs[i] = 2 * inter / (union + epsilon)\n        \n    return np.sum(dice_coeffs)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-08T20:21:12.199125Z","iopub.execute_input":"2023-12-08T20:21:12.199442Z","iopub.status.idle":"2023-12-08T20:21:12.223971Z","shell.execute_reply.started":"2023-12-08T20:21:12.199417Z","shell.execute_reply":"2023-12-08T20:21:12.223057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_folders = list_files_in_directory(os.path.join(CFG.data_dir, \"train\"))\ntest_folders = list_files_in_directory(os.path.join(CFG.data_dir, \"test\"))\ntrain_file_dir = count_total_img(train_folders)\ntrain_folders.remove(\"/kaggle/input/blood-vessel-segmentation/train/kidney_3_dense\")\nprint(train_folders)\n# test_file_dir = count_total_img(test_folders)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:12.225219Z","iopub.execute_input":"2023-12-08T20:21:12.225606Z","iopub.status.idle":"2023-12-08T20:21:17.071656Z","shell.execute_reply.started":"2023-12-08T20:21:12.225573Z","shell.execute_reply":"2023-12-08T20:21:17.070732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_random_img(train_file_dir)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:17.072987Z","iopub.execute_input":"2023-12-08T20:21:17.073371Z","iopub.status.idle":"2023-12-08T20:21:18.022638Z","shell.execute_reply.started":"2023-12-08T20:21:17.073337Z","shell.execute_reply":"2023-12-08T20:21:18.021725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Construct datasets from image data","metadata":{}},{"cell_type":"code","source":"class VesselDataset(Dataset):\n    @classmethod\n    def load_image(cls, image_path: str):\n        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n        img = np.tile(img[...,None], [1, 1, 3])          # gray to rgb\n        img = img.astype('float32')\n        mx = np.max(img)\n        if mx:\n            img/=mx # scale image to [0, 1]\n        return img\n        \n    @classmethod\n    def load_mask(cls, mask_path: str):\n        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n        mask = mask.astype('float32')\n        mask /= 255.0\n        return mask\n    \n    \n    def __init__(self, data_dir: str, mode: str, transforms=None):\n        assert(mode == \"train\" or mode == \"test\")\n        self.mode = mode\n        self.image_dir = os.path.join(data_dir, \"images\")\n        self.image_files = os.listdir(self.image_dir)\n        if mode == \"train\":\n            self.mask_dir = os.path.join(data_dir, \"labels\")\n            self.mask_files = os.listdir(self.mask_dir)\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx: int):\n        image_path = os.path.join(self.image_dir, self.image_files[idx])\n        image = VesselDataset.load_image(image_path)\n        \n        if self.mode == \"train\":\n            mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n            mask = VesselDataset.load_mask(mask_path)\n            if self.transforms:\n                data = self.transforms(image=image, mask=mask)\n                image, mask = data[\"image\"], data[\"mask\"]\n            image = np.transpose(image, (2, 0, 1))\n            return torch.tensor(image), torch.tensor(mask)\n        else:\n            shape = image.shape\n            if self.transforms:\n                image = self.transforms(image=image)[\"image\"]\n            image = np.transpose(image, (2, 0, 1))\n            return torch.tensor(image), torch.tensor(np.array([shape[0], shape[1]]))\n\n\nclass DiceCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super().__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n#         print(\"inputs: \", inputs.shape)\n#         print(\"targets: \", targets.shape)\n        \n        inputs = inputs.reshape(-1)\n        targets = targets.reshape(-1)\n        \n        # Dice Loss\n        intersection = (inputs * targets).sum()\n        dice_loss = 1 - (2.0 * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n        \n        # Cross Entropy Loss\n#         CE = F.cross_entropy(inputs, targets, reduction='mean')\n        \n        # Combine Dice and BCE\n#         Dice_CE = CE + dice_loss\n        return dice_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:18.023853Z","iopub.execute_input":"2023-12-08T20:21:18.024137Z","iopub.status.idle":"2023-12-08T20:21:18.041392Z","shell.execute_reply.started":"2023-12-08T20:21:18.024112Z","shell.execute_reply":"2023-12-08T20:21:18.040499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unet model","metadata":{}},{"cell_type":"code","source":"train_groups = [os.path.join(CFG.data_dir, \"train\", folder) for folder in CFG.train_groups]\nval_groups = [os.path.join(CFG.data_dir, \"train\", folder) for folder in CFG.valid_groups]\n\ntrain_dataset = VesselDataset(train_groups[0], mode=\"train\", transforms=CFG.data_augmentation['default'])\nfor idx in range(1, len(train_groups)):\n    train_dataset += VesselDataset(train_groups[idx], mode=\"train\", transforms=CFG.data_augmentation['default'])\n\nvalid_dataset = VesselDataset(val_groups[0], mode=\"train\", transforms=CFG.data_transforms['valid'])\nfor idx in range(1, len(val_groups)):\n    valid_dataset += VesselDataset(val_groups[idx], mode=\"train\", transforms=CFG.data_transforms['valid'])","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:18.042805Z","iopub.execute_input":"2023-12-08T20:21:18.043575Z","iopub.status.idle":"2023-12-08T20:21:18.066053Z","shell.execute_reply.started":"2023-12-08T20:21:18.043539Z","shell.execute_reply":"2023-12-08T20:21:18.065137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, criterion, optimizer, dataloader, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(CFG.device, dtype=torch.float)\n        masks  = masks.to(CFG.device, dtype=torch.float)\n        batch_size = images.size(0)\n        \n        optimizer.zero_grad()\n        masks_pred = model(images)\n        loss = criterion(masks_pred.squeeze(), masks)\n        loss.backward()\n        optimizer.step()\n        \n        if step % 10 == 0:\n            print(f\"Step {step}, Loss: {loss}\")\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n    epoch_loss = running_loss / dataset_size\n\n    mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n    current_lr = optimizer.param_groups[0]['lr']\n    pbar.set_postfix( epoch=f'{epoch}',\n                      train_loss=f'{epoch_loss:0.4f}',\n                      lr=f'{current_lr:0.5f}',\n                      gpu_mem=f'{mem:0.2f} GB')\n    torch.cuda.empty_cache()\n    gc.collect()\n    return epoch_loss\n\n@torch.no_grad()\ndef valid_one_epoch(model, criterion, dataloader):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    dice_coeff = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:\n        images  = images.to(CFG.device, dtype=torch.float)\n        masks   = masks.to(CFG.device, dtype=torch.float)\n        \n        with torch.no_grad():\n            batch_size = images.size(0)\n            masks_pred = model(images)\n            loss = criterion(masks_pred.squeeze(), masks)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            dice_coeff += dice_coeff_batch(masks_pred.to(\"cpu\").numpy().squeeze(), masks.to(\"cpu\").numpy().squeeze())\n        \n    epoch_loss = running_loss / dataset_size\n    epoch_dice_coeff = dice_coeff / dataset_size\n    mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n    current_lr = optimizer.param_groups[0]['lr']\n    pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                    lr=f'{current_lr:0.5f}',\n                    gpu_memory=f'{mem:0.2f} GB')\n#     val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n    return epoch_loss, epoch_dice_coeff\n\ndef train(model, criterion, optimizer, num_epochs, model_name, train_loader, valid_loader):\n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n\n    start_time = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss      = np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n\n    for epoch in range(1, num_epochs + 1):\n        gc.collect()\n        start = time.time()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = train_one_epoch(\n            model,\n            criterion,\n            optimizer, \n            dataloader=train_loader, \n            epoch=epoch\n        )\n        \n        val_loss, dice_coeff = valid_one_epoch(\n            model,\n            criterion,\n            valid_loader\n        )\n#         val_dice, val_jaccard = val_scores\n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        history[\"Dice Coeff\"].append(dice_coeff)\n#         history['Valid Dice'].append(val_dice)\n#         history['Valid Jaccard'].append(val_jaccard)\n#         print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n        print(f'Valid Loss: {val_loss}')\n    \n        # deep copy the model\n        if val_loss <= best_loss:\n            print(f\"Valid loss Improved ({best_loss} ---> {val_loss})\")\n#             best_dice = val_dice\n#             best_jaccard = val_jaccard\n            best_loss = val_loss\n            best_epoch = epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"/kaggle/working/best_epoch.bin\"\n            torch.save(model.state_dict(), PATH)\n            print(f\"Model Saved to: {PATH}\")\n            \n        epoch_duration = time.time() - start\n        print(\"Epoch finish in {:.0f}h {:.0f}m {:.0f}s\".format(\n            epoch_duration // 3600, (epoch_duration % 3600) // 60, (epoch_duration % 3600) % 60)\n        )\n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = \"last_epoch.bin\"\n        torch.save(model.state_dict(), PATH)\n\n    end_time = time.time()\n    time_elapsed = end_time - start_time\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60)\n    )\n    print(\"Best Loss: {:.4f}\".format(best_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    torch.save(model.state_dict(), os.path.join(CFG.saved_model_path, model_name))\n    print(f\"Model saved to {os.path.join(CFG.saved_model_path, model_name)}\")\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:18.070808Z","iopub.execute_input":"2023-12-08T20:21:18.071166Z","iopub.status.idle":"2023-12-08T20:21:18.098851Z","shell.execute_reply.started":"2023-12-08T20:21:18.071135Z","shell.execute_reply":"2023-12-08T20:21:18.097721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import albumentations as A\n# from albumentations.pytorch import ToTensorV2\n\n\n# DATASET_FOLDER = '/kaggle/input/blood-vessel-segmentation'\n# IMG_PATH = DATASET_FOLDER + '/train'\n# TO_VISUALIZE = 'kidney_1_dense'\n\n# df_train = pd.read_csv(os.path.join(DATASET_FOLDER, \"train_rles.csv\"))\n# df_train[[\"dataset\", \"slice\"]] = df_train['id'].str.rsplit(pat='_', n=1, expand=True)\n\n# valid_transforms = A.Compose([\n#     A.HorizontalFlip(p=0.5),\n#     A.RandomRotate90(p=0.5),\n#     A.ToRGB(),\n#     ToTensorV2()\n# ])\n\n# train_dataset = SenNetHOATiledDataset(\n#                             df_train.loc[df_train.dataset == TO_VISUALIZE],\n#                             path_img_dir=IMG_PATH,\n#                             tile_size=[800, 800],\n#                             empty_tile_pct=0.0,\n#                             transforms=valid_transforms)\n\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=8,\n#     num_workers=4,\n#     shuffle=False,\n#     pin_memory=True\n# )","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:18.100042Z","iopub.execute_input":"2023-12-08T20:21:18.100443Z","iopub.status.idle":"2023-12-08T20:21:18.113873Z","shell.execute_reply.started":"2023-12-08T20:21:18.100389Z","shell.execute_reply":"2023-12-08T20:21:18.112932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model = smp.Unet(\n    encoder_name=\"resnet50\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    activation=None,\n)\nunet_model.to(CFG.device)\n\nlr = 1e-2\nnum_epoch = 4\n# model.load_state_dict(torch.load(checkpoint_path))\ncriterion = smp.losses.DiceLoss(mode='binary')\noptimizer = optim.AdamW(unet_model.parameters(), lr=lr)\n\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, shuffle=False)\n\nmodel, history = train(\n    unet_model,\n    criterion,\n    optimizer,\n    num_epochs=num_epoch, \n    model_name=f\"unet-epoch{num_epoch}-lr{lr}.pth\",\n    train_loader=train_loader,\n    valid_loader=valid_loader\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:21:18.115024Z","iopub.execute_input":"2023-12-08T20:21:18.115360Z","iopub.status.idle":"2023-12-08T20:54:24.176148Z","shell.execute_reply.started":"2023-12-08T20:21:18.115323Z","shell.execute_reply":"2023-12-08T20:54:24.174194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sanity Check","metadata":{}},{"cell_type":"code","source":"images, masks = next(iter(valid_loader))\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_epoch.bin\"))\n\nwith torch.no_grad():\n    images  = images.to(CFG.device, dtype=torch.float)\n    masks   = masks.to(CFG.device, dtype=torch.float)\n    masks_pred = model(images)\n    \n    loss = criterion(masks_pred, masks)\n    masks_pred = (nn.Sigmoid()(masks_pred) > 0.9).double()\n    masks = masks.to(\"cpu\").numpy().squeeze()\n    masks_pred = masks_pred.to(\"cpu\").numpy().squeeze()\n\n    print(\"loss: \", loss)\nfor i in range(4):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 15))\n    axes[0].set_title(\"Predicted Mask\")\n    axes[0].imshow(masks_pred[i])\n    \n    axes[1].set_title(\"True Mask\")\n    axes[1].imshow(masks[i])","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:24.181138Z","iopub.execute_input":"2023-12-08T20:54:24.181458Z","iopub.status.idle":"2023-12-08T20:54:28.914211Z","shell.execute_reply.started":"2023-12-08T20:54:24.181431Z","shell.execute_reply":"2023-12-08T20:54:28.913232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = f\"unet-epoch{num_epoch}-lr{lr}.pth\"\nmodel_path = os.path.join(dirname(CFG.data_dir), \"saved-models\", model_name)\nunet_model.load_state_dict(torch.load(model_path))\n\ntest_loader = DataLoader(test_dataset, batch_size=CFG.valid_bs, shuffle=False)\nit = iter(test_loader)\nimages, masks = next(it)\nimages = images.to(CFG.device, dtype=torch.float)\nmasks = masks.to(CFG.device, dtype=torch.float)\nwith amp.autocast(enabled=True):\n    masks_pred = unet_model(images)\n    \n    \nfor i in range(len(masks_pred)):\n    pred = masks_pred[i]\n    pred_np = pred.cpu().detach().numpy()\n#     real_np = real.cpu().detach().numpy()\n    image = images[i]\n    image_np = image.cpu().detach().numpy()\n    fig, axes = plt.subplots(1,2)\n    axes[0].imshow(np.transpose(np.array(image_np, dtype=\"uint8\"), (1, 2, 0)))\n    axes[0].set_title(f\"Input Image {i + 1}\")\n    axes[1].imshow(np.transpose(pred_np, (1, 2, 0)))\n    axes[1].set_title(f\"Predicted Mask for image {i + 1}\")\n#     axes[2].imshow(real_np)\n#     axes[2].set_title(f\"Real Mask for image {i + 1}\")\n    fig.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:28.915461Z","iopub.execute_input":"2023-12-08T20:54:28.915748Z","iopub.status.idle":"2023-12-08T20:54:29.878898Z","shell.execute_reply.started":"2023-12-08T20:54:28.915714Z","shell.execute_reply":"2023-12-08T20:54:29.877580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SAM Model","metadata":{}},{"cell_type":"markdown","source":"#### Predicted mask size for SAM model will always be (256, 256), and SAM processor will transform input image to size (1024, 1024) since it uses ViT as image encoder (future -> we could try to replace image encoder with Swin Transformer instead to take abitrary inputs)","metadata":{}},{"cell_type":"code","source":"def resize_mask(image):\n    longest_edge = 256\n    \n    # get new size\n    w, h = image.size\n    scale = longest_edge * 1.0 / max(h, w)\n    new_h, new_w = h * scale, w * scale\n    new_h = int(new_h + 0.5)\n    new_w = int(new_w + 0.5)\n\n    resized_image = image.resize((new_w, new_h), resample=Image.Resampling.BILINEAR)\n    return resized_image\n\ndef pad_mask(image):\n    pad_height = 256 - image.height\n    pad_width = 256 - image.width\n\n    padding = ((0, pad_height), (0, pad_width))\n    padded_image = np.pad(image, padding, mode=\"constant\")\n    return padded_image\n\ndef process_mask(image):\n    resized_mask = resize_mask(image)\n    padded_mask = pad_mask(resized_mask)\n    return padded_mask","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.879905Z","iopub.status.idle":"2023-12-08T20:54:29.880267Z","shell.execute_reply.started":"2023-12-08T20:54:29.880102Z","shell.execute_reply":"2023-12-08T20:54:29.880118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch.nn.functional as F\nfrom torch.nn.functional import threshold, normalize\nfrom typing import Tuple\n\ndef postprocess_masks(masks: torch.Tensor, input_size: Tuple[int, ...], original_size: Tuple[int, ...],\n                      image_size=1024) -> torch.Tensor:\n    \"\"\"\n    Remove padding and upscale masks to the original image size.\n\n    Args:\n      masks (torch.Tensor):\n        Batched masks from the mask_decoder, in BxCxHxW format.\n      input_size (tuple(int, int)):\n        The size of the image input to the model, in (H, W) format. Used to remove padding.\n      original_size (tuple(int, int)):\n        The original size of the image before resizing for input to the model, in (H, W) format.\n\n    Returns:\n      (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n        is given by original_size.\n    \"\"\"\n    masks = F.interpolate(\n        masks,\n        (image_size, image_size),\n        mode=\"bilinear\",\n        align_corners=False,\n    )\n    masks = masks[..., : input_size[0], : input_size[1]]\n    masks = F.interpolate(masks, original_size, mode=\"bilinear\", align_corners=False)\n    return masks\n\ndef get_model_response(model, inputs, input_resized_shape, input_original_shape):\n    with torch.no_grad():\n        outputs = model(**inputs, multimask_output=False)\n    processed_output = postprocess_masks(outputs[\"pred_masks\"][0], input_resized_shape, input_original_shape)\n    predicted_masks = normalize(threshold(processed_output, 0.0, 0)).squeeze(1)\n    return predicted_masks[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.882128Z","iopub.status.idle":"2023-12-08T20:54:29.882581Z","shell.execute_reply.started":"2023-12-08T20:54:29.882353Z","shell.execute_reply":"2023-12-08T20:54:29.882375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import SamModel, SamProcessor, AutoProcessor, AutoModelForMaskGeneration\nfrom PIL import Image\n\nclass SAMDataset(Dataset):\n    @classmethod\n    def get_bounding_box(cls, ground_truth_map):\n        '''\n        This function creates varying bounding box coordinates based on the segmentation contours as prompt for the SAM model\n        The padding is random int values between 5 and 20 pixels\n        '''\n        # get bounding box from mask\n        y_indices, x_indices = np.where(ground_truth_map > 0)\n        x_min, x_max = np.min(x_indices), np.max(x_indices)\n        y_min, y_max = np.min(y_indices), np.max(y_indices)\n\n        # add perturbation to bounding box coordinates\n        H, W = ground_truth_map.shape\n        x_min = max(0, x_min - np.random.randint(5, 20))\n        x_max = min(W, x_max + np.random.randint(5, 20))\n        y_min = max(0, y_min - np.random.randint(5, 20))\n        y_max = min(H, y_max + np.random.randint(5, 20))\n\n        bbox = [x_min, y_min, x_max, y_max]\n\n        return bbox\n    \n    @classmethod\n    def load_image(cls, image_path: str):\n        img = Image.open(image_path)\n        if img.mode != 'RGB':\n            img = img.convert('RGB')\n        img = np.array(img)\n        img = img.astype('float32')\n        img /= 255.0                                     # image normalization                 \n        return img\n\n    @classmethod\n    def load_mask(cls, mask_path: str):\n        mask = Image.open(mask_path)\n        # resize mask to (256, 256)\n        mask = process_mask(mask)\n        mask = np.array(mask)\n        mask = mask.astype('float32')\n        mask /= 255.0\n        return mask\n    \n    def __init__(self, data_dir: str, mode: str, processor: SamProcessor, transforms=None):\n        assert(mode == \"train\" or mode == \"test\")\n        self.mode = mode\n        self.image_dir = os.path.join(data_dir, \"images\")\n        self.image_files = os.listdir(self.image_dir)\n        self.mask_dir = os.path.join(data_dir, \"labels\")\n        self.mask_files = os.listdir(self.mask_dir)\n        self.transforms = transforms\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx: int):\n        image_path = os.path.join(self.image_dir, self.image_files[idx])\n        image = SAMDataset.load_image(image_path)\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]\n        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n        mask = SAMDataset.load_mask(mask_path)\n        \n        if self.mode == \"train\":\n            image = np.transpose(image, (2, 0, 1))\n            input_boxes = SAMDataset.get_bounding_box(mask)\n            inputs = self.processor(image, input_boxes=[[input_boxes]], return_tensors=\"pt\")\n            inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n            inputs[\"ground_truth_mask\"] = mask\n            return inputs\n        else:\n            # put input_boxes as the same size of inputs\n            image = np.transpose(image, (2, 0, 1))\n            input_boxes = [0, 0, image.shape[-2], image.shape[-1]]\n            inputs = self.processor(image, input_boxes=[[input_boxes]], return_tensors=\"pt\")\n            inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n            inputs[\"ground_truth_mask\"] = mask\n            return inputs\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.884126Z","iopub.status.idle":"2023-12-08T20:54:29.884573Z","shell.execute_reply.started":"2023-12-08T20:54:29.884344Z","shell.execute_reply":"2023-12-08T20:54:29.884366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.profiler import Profiler\n\ndef pt_to_pil_convertor(image):\n    image = np.transpose(image.squeeze().numpy(), (1, 2, 0))\n    converted_img = Image.fromarray((image * 255).astype(np.uint8))\n    return converted_img\n\ndef dice_coeff_batch(predictions, targets):\n    # Initialize an array to store dice coefficients for each pair in the batch\n    dice_coeffs = np.zeros(predictions.shape[0])\n\n    for i in range(predictions.shape[0]):\n        # Extract the single prediction and target pair from the batch\n        prediction = predictions[i]\n        target = targets[i]\n\n        # Create a binary mask for the prediction\n        mask = np.zeros_like(prediction)\n        mask[prediction >= 0.5] = 1\n\n        # Compute intersection and union\n        inter = np.sum(mask * target)\n        union = np.sum(mask) + np.sum(target)\n\n        # Add a small epsilon to avoid division by zero\n        epsilon = 1e-6\n        # Compute Dice coefficient for the current pair\n        dice_coeffs[i] = 2 * inter / (union + epsilon)\n        \n    return np.sum(dice_coeffs)\n\ndef build_model(model_name: str):\n    processor = AutoProcessor.from_pretrained(model_name)\n    model = AutoModelForMaskGeneration.from_pretrained(model_name)\n    \n    # Freeze all parameters in the model except mask decoder\n    for param in model.parameters():\n        param.requires_grad = False\n    for param in model.mask_decoder.parameters():\n        param.requires_grad = True\n    \n#     model = nn.DataParallel(model)\n    model.to(CFG.device)\n    return model, processor\n\ndef finetune_sam_one_epoch(\n    model: SamModel,\n    criterion,\n    optimizer,\n    dataloader: DataLoader,\n    epoch: int\n):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    dice_coeff = 0.0\n\n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Finetune \")\n    for step, batch in pbar:\n#         convertor = lambda x: pt_to_pil_convertor(x)\n#         converted_imgs = np.vectorize(convertor)(images)\n#         masks = masks.to(CFG.device, dtype=torch.float)\n#         input_boxes = [input_boxes]\n#         inputs = processor(converted_imgs, input_boxes=input_boxes, return_tensors=\"pt\").to(CFG.device)\n        batch_size = batch[\"pixel_values\"].size(0)\n        with amp.autocast(enabled=True):\n            outputs = model(\n                pixel_values=batch[\"pixel_values\"].to(CFG.device),\n                input_boxes=batch[\"input_boxes\"].to(CFG.device),\n                multimask_output=False\n            )\n            \n            predicted_masks = outputs.pred_masks.squeeze(1)\n            ground_truth_masks = batch[\"ground_truth_mask\"].to(CFG.device)\n            loss = criterion(predicted_masks, ground_truth_masks.unsqueeze(1))\n            loss = loss / CFG.n_accumulate\n        \n            scaler.scale(loss).backward()\n\n            if (step + 1) % CFG.n_accumulate == 0:\n                scaler.step(optimizer)\n                scaler.update()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n            mem = torch.cuda.memory_reserved() / 1E9 if CFG.device else 0\n            current_lr = optimizer.param_groups[0]['lr']\n            pbar.set_postfix( epoch=f'{epoch}',\n                              train_loss=f'{epoch_loss:0.4f}',\n                              lr=f'{current_lr:0.5f}',\n                              gpu_mem=f'{mem:0.2f} GB')\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return epoch_loss\n\n@torch.no_grad()\ndef valid_sam_one_epoch(\n    model: SamModel,\n    criterion,\n    dataloader: DataLoader\n):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    dice_coeff = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Valid \")\n    for step, batch in pbar:\n        batch_size = batch[\"pixel_values\"].size(0)\n        with torch.no_grad():\n            outputs = model(\n                pixel_values=batch[\"pixel_values\"].to(CFG.device),\n                input_boxes=batch[\"input_boxes\"].to(CFG.device),\n                multimask_output=False\n            )\n\n            predicted_masks = outputs.pred_masks.squeeze(1)\n            ground_truth_masks = batch[\"ground_truth_mask\"].to(CFG.device)\n            loss = criterion(predicted_masks, ground_truth_masks.unsqueeze(1))\n\n            mask_pred = predicted_masks.cpu().numpy()\n            mask_true = ground_truth_masks.unsqueeze(1).cpu().numpy()\n            dice_coeff += dice_coeff_batch(mask_pred, mask_true)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n    \n    epoch_loss = running_loss / dataset_size\n    epoch_dice_coeff = dice_coeff / dataset_size\n    mem = torch.cuda.memory_reserved() / 1E9 if CFG.device else 0\n    current_lr = optimizer.param_groups[0]['lr']\n    pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                    lr=f'{current_lr:0.5f}',\n                    gpu_memory=f'{mem:0.2f} GB')\n    torch.cuda.empty_cache()\n    gc.collect()\n    return epoch_loss, epoch_dice_coeff\n\n\ndef finetune_medsam(\n    model: SamModel,\n    criterion,\n    optimizer,\n    num_epochs: int,\n    model_name: str,\n    train_loader: DataLoader,\n    valid_loader: DataLoader\n):\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    start_time = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss      = np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n\n    for epoch in range(1, num_epochs + 1):\n        gc.collect()\n        start = time.time()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = finetune_sam_one_epoch(\n            model,\n            criterion,\n            optimizer,\n            dataloader=train_loader, \n            epoch=epoch\n        )\n        \n        val_loss = valid_sam_one_epoch(\n            model,\n            criterion,\n            valid_loader\n        )\n        \n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        print(f'Valid Loss: {val_loss}')\n    \n        # deep copy the model\n        if val_loss <= best_loss:\n            print(f\"Valid loss Improved ({best_loss} ---> {val_loss})\")\n            best_loss = val_loss\n            best_epoch = epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"/kaggle/working/best_epoch.bin\"\n            torch.save(model.state_dict(), PATH)\n            print(f\"Model Saved to: {PATH}\")\n            \n        epoch_duration = time.time() - start\n        print(\"Epoch finish in {:.0f}h {:.0f}m {:.0f}s\".format(\n            epoch_duration // 3600, (epoch_duration % 3600) // 60, (epoch_duration % 3600) % 60)\n        )\n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = \"last_epoch.bin\"\n        torch.save(model.state_dict(), PATH)\n\n    end_time = time.time()\n    time_elapsed = end_time - start_time\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60)\n    )\n    print(\"Best Loss: {:.4f}\".format(best_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    torch.save(model.state_dict(), os.path.join(CFG.saved_model_path, model_name))\n    print(f\"Model saved to {os.path.join(CFG.saved_model_path, model_name)}\")\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.886336Z","iopub.status.idle":"2023-12-08T20:54:29.886674Z","shell.execute_reply.started":"2023-12-08T20:54:29.886510Z","shell.execute_reply":"2023-12-08T20:54:29.886527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sam_model, processor = build_model(\"wanglab/medsam-vit-base\")\n\n# train_groups = [os.path.join(CFG.data_dir, \"train\", folder) for folder in CFG.train_groups]\n# val_groups = [os.path.join(CFG.data_dir, \"train\", folder) for folder in CFG.valid_groups]\n\n\n# train_dataset = SAMDataset(train_groups[0], mode=\"train\", processor=processor, transforms=CFG.sam_transformations[\"default\"])\n# for idx in range(1, len(train_groups)):\n#     train_dataset += SAMDataset(train_groups[idx], mode=\"train\", processor=processor, transforms=CFG.sam_transformations[\"default\"])\n\n# valid_dataset = SAMDataset(val_groups[0], mode=\"test\", processor=processor, transforms=CFG.sam_transformations[\"default\"])\n# for idx in range(1, len(val_groups)):\n#     valid_dataset += SAMDataset(val_groups[idx], mode=\"test\", processor=processor, transforms=CFG.sam_transformations[\"default\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.887757Z","iopub.status.idle":"2023-12-08T20:54:29.888089Z","shell.execute_reply.started":"2023-12-08T20:54:29.887926Z","shell.execute_reply":"2023-12-08T20:54:29.887942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processor.from_pretrained(\"/kaggle/working/preprocessor_config.json\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.889311Z","iopub.status.idle":"2023-12-08T20:54:29.889659Z","shell.execute_reply.started":"2023-12-08T20:54:29.889492Z","shell.execute_reply":"2023-12-08T20:54:29.889510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, shuffle=True)\n# valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, shuffle=False)\n\n# batch = next(iter(valid_loader))\n# for k, v in batch.items():\n#     print(k, v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.890873Z","iopub.status.idle":"2023-12-08T20:54:29.891181Z","shell.execute_reply.started":"2023-12-08T20:54:29.891024Z","shell.execute_reply":"2023-12-08T20:54:29.891038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sam_lr = 1e-3\n# criterion = DiceCELoss()\n# # criterion = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n# optimizer = optim.AdamW(sam_model.mask_decoder.parameters(), lr=sam_lr)\n\n# model, history = finetune_medsam(\n#     sam_model,\n#     criterion, \n#     optimizer, \n#     num_epochs=5,\n#     model_name=\"medsam-finetuned\",\n#     train_loader=train_loader,\n#     valid_loader=valid_loader\n# )\n\n# # masks = processor.image_processor.post_process_masks(\n# #     outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n# # )\n\n# # data agumentation: multiple bbox prompts, how about langauge promots? how to maintain its zero-shot capacity?\n# # distillation\n# # scores = outputs.iou_scores","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.892531Z","iopub.status.idle":"2023-12-08T20:54:29.892890Z","shell.execute_reply.started":"2023-12-08T20:54:29.892716Z","shell.execute_reply":"2023-12-08T20:54:29.892737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Med SAM uses DiceCELoss loss so we need to postprocess generated masks","metadata":{}},{"cell_type":"code","source":"# def show_mask(pred_masks, ax):\n#     # apply sigmoid\n#     seg_prob = torch.sigmoid(pred_masks.squeeze(1))\n#     # convert soft mask to hard mask\n#     seg_prob = seg_prob.to(\"cpu\").numpy().squeeze()\n#     seg = (seg_prob > 0.5).astype(np.uint8)\n#     h, w = mask.shape[-2:]\n#     mask_image = mask.reshape(h, w, 1)\n#     ax.imshow(mask_image)\n#     return mask_image\n\n# def display_history(history):\n#     fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n#     train_loss = history[\"Train Loss\"]\n#     valid_loss = history[\"Valid Loss\"]\n#     dice_coeff = history[\"Dice Coeff\"]\n\n#     epochs = range(1, len(train_loss) + 1)\n\n#     # Plotting Train Loss\n#     axes[0].plot(epochs, train_loss, 'r-', label='Train Loss')\n#     axes[0].set_title('Training Loss')\n#     axes[0].set_xlabel('Epochs')\n#     axes[0].set_ylabel('Loss')\n#     axes[0].legend()\n\n#     # Plotting Validation Loss\n#     axes[1].plot(epochs, valid_loss, 'b-', label='Validation Loss')\n#     axes[1].set_title('Validation Loss')\n#     axes[1].set_xlabel('Epochs')\n#     axes[1].set_ylabel('Loss')\n#     axes[1].legend()\n\n#     # Plotting Dice Coefficient\n#     axes[2].plot(epochs, dice_coeff, 'g-', label='Dice Coefficient')\n#     axes[2].set_title('Dice Coefficient')\n#     axes[2].set_xlabel('Epochs')\n#     axes[2].set_ylabel('Dice Coeff')\n#     axes[2].legend()\n\n#     plt.tight_layout()\n#     plt.show()\n\n# # fig, axes = plt.subplots(1, 2)\n# # mask_np = np.transpose(mask.numpy(), (1, 2, 0))\n# # axes[0].imshow(mask_np)\n# # axes[0].set_title(f\"True mask\")\n# # pred_mask = show_mask(outputs.pred_masks, axes[1])\n# # axes[1].set_title(f\"Predicted mask\")\n\n\n# # need to resize to 1024x1024 (input image)\n# # ground truth masks needed to \n\n# display_history(history)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T20:54:29.894298Z","iopub.status.idle":"2023-12-08T20:54:29.894633Z","shell.execute_reply.started":"2023-12-08T20:54:29.894465Z","shell.execute_reply":"2023-12-08T20:54:29.894480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ground truth masks vs. predicted masks","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}